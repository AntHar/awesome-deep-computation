
# Awesome Deep Computation [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)
A curated list of awesome deep learning hardware, compute cycle/memory optimisation and implementation techniques.  Inspired by [awesome-deep-learning](https://github.com/ChristosChristofidis/awesome-deep-learning).
Literature from 2014 onwards.


#### Transistor/Gate Level Hardware

1. 2016/05 [A 2.2 GHz SRAM with High Temperature Variation Immunity for Deep Learning Application under 28nm](http://dl.acm.org/citation.cfm?id=2903982)
2. 2016/06 [Switched by Input: Power Efficient Structure for RRAM-based Convolutional Neural Network](https://nicsefc.ee.tsinghua.edu.cn/media/publications/2016/DAC16_197.pdf)
3. 2016/06 [Low-power approximate convolution computing unit with domain-wall motion based "spin-memristor" for image processing applications](http://dl.acm.org/citation.cfm?id=2898042)

#### Low Level Hardware Architecture

1.  2014/06 [A 240 G-ops/s Mobile Coprocessor for Deep Neural Networks](http://www.cv-foundation.org//openaccess/content_cvpr_workshops_2014/W17/papers/Gokhale_A_240_G-opss_2014_CVPR_paper.pdf)
2.  2015/02 [Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks](http://cadlab.cs.ucla.edu/~cong/slides/fpga2015_chen.pdf)
3.  2015/06 [Neuromorphic Architectures for Spiking Deep Neural Networks](http://ncs.ethz.ch/pubs/pdf/Indiveri_etal15.pdf)
4.  2015/06 [Memory and information processing in neuromorphic systems](http://arxiv.org/pdf/1506.03264.pdf)
5.  2015/08 [INsight: A Neuromorphic Computing System for Evaluation of Large Neural Networks](http://arxiv.org/pdf/1508.01008.pdf)
6.  2016/02 [Deep Learning on FPGAs: Past, Present, and Future.](http://arxiv.org/abs/1602.04283)
7.  2016/02 [A 1.42TOPS/W deep convolutional neural network recognition processor for intelligent IoE systems](http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=7418008&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D7418008)
8.  2016/02 [vDNN: Virtualized Deep Neural Networks for Scalable, Memory-Efficient Neural Network Design](http://arxiv.org/pdf/1602.08124v3.pdf)
9.  2016/04 [Demonstrating Hybrid Learning in a Flexible Neuromorphic Hardware System](https://arxiv.org/pdf/1604.05080.pdf)
10.  2016/04 [Hardware-oriented Approximation of Convolutional Neural Networks](http://arxiv.org/pdf/1604.03168v2.pdf)
11.  2016/04 [Acceleration of Deep Neural Network Training with Resistive Cross-Point Devices](https://arxiv.org/pdf/1603.07341.pdf)
12.  2016/05 [ISAAC: A Convolutional Neural Network Accelerator with In-Situ Analog Arithmetic in Crossbars](https://www.cs.utah.edu/~rajeev/pubs/isca16.pdf)
13.  2016/07 [Maximizing CNN Accelerator Efficiency Through Resource Partitioning](http://arxiv.org/pdf/1607.00064v1.pdf)
14.  2016/07 [Overcoming Resource Underutilization in Spatial CNN Accelerators](http://compas.cs.stonybrook.edu/~mferdman/downloads.php/FPL16_Overcoming_Resource_Underutilization_in_Spatial_CNN_Accelerators.pdf)
15.  2016/07 [Dynamic energy-accuracy trade-off using stochastic computing in deep neural networks](http://dl.acm.org/citation.cfm?id=2898011)

#### Model Implementation Techniques

1.  2014/12 [Training Deep Neural Neworks with Low Precision Multiplications](https://arxiv.org/pdf/1412.7024.pdf)
2.  2014/12 [Implementation of Deep Convolutional Neural Net on a Digital Signal Processor](http://cs229.stanford.edu/proj2014/Elaina%20Chai,Implementation%20of%20Deep%20Convolutional%20NeuralNet%20on%20a%20DSP.pdf)
3.  2015/02 [Deep Learning with Limited Numerical Precision](https://arxiv.org/pdf/1502.02551.pdf)
4.  2015/02 [Faster learning of deep stacked autoencoders on multi-core systems using synchronized layer-wise pre-training](https://arxiv.org/abs/1603.02836)
5.  2015/02 [8-Bit Approximations for Parallelism in Deep Learning](http://arxiv.org/pdf/1511.04561v4.pdf)
6.  2016/01 [DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients](http://arxiv.org/pdf/1606.06160v2.pdf)
7.  2016/02 [Neural Networks with Few Multiplications](https://arxiv.org/abs/1510.03009)
8.  2016/02 [Deep Compression: Compressing Deep Neural Networks with Pruning, Quantization and Huffman Coding](http://arxiv.org/abs/1510.00149)
9.  2016/02 [8-Bit Approximations for Parallelism in Deep Learning](http://arxiv.org/abs/1511.04561)


#### Tutorials and talks

1.  2015/09 [Heterogeneous Computing in HPC and Deep Learning](https://hpcuserforum.com/presentations/colorado-sept2015/InspuHeterogeneousComputingInHPCandDeepLearning.pdf)
2.  2016/02 [Going Deeper with Embedded FPGA Platform for Convolutional Neural Network](http://www.isfpga.org/index_files/Slides/1_2.pdf)
3.  2016/02 [Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks](http://www.rle.mit.edu/eems/wp-content/uploads/2016/02/eyeriss_isscc_2016_slides.pdf)
4.  2016/05 [Deep Compression: Compressing Deep Neural Networks with Pruning, Quantization and Huffman Coding](http://videolectures.net/iclr2016_han_deep_compression/)
5.  2016/05 [DNNWEAVER: From High-Level Deep Network Models to FPGA Acceleration](http://www.cc.gatech.edu/~hadi/doc/paper/2016-cogarch-dnn_weaver.pdf)

#### Thesis

1. 2015/08 [FPGA based Multi-core architectures for Deep Learning](https://etd.ohiolink.edu/!etd.send_file?accession=dayton1449417091&disposition=inline)
1. 2016/05 [Ristretto:  Hardware-Oriented Approximation of Convolutional Neural Networks](https://arxiv.org/pdf/1605.06402.pdf)

#### Whitepapers

1.  2015/02 [Accelerating Deep Convolutional Neural Networks Using Specialized Hardware](http://research.microsoft.com/pubs/240715/CNN%20Whitepaper.pdf)
2.  2015/07 [Efficient Implementation of Neural Network Systems Built on FPGAs, Programmed with OpenCL](https://www.altera.com/en_US/pdfs/literature/solution-sheets/efficient_neural_networks.pdf)

#### Blogs and Articles

1.  2015/05 [Numerical Optimization for Deep Learning](http://insidehpc.com/2015/05/numerical-optimization-for-deep-learning/)
2.  2015/10 [Single Node Caffe Scoring and Training on IntelÂ® Xeon E5-Series Processors](https://software.intel.com/en-us/articles/single-node-caffe-scoring-and-training-on-intel-xeon-e5-series-processors)
3.  2016/03 [FPGAs Challenge GPUs as a Platform for Deep Learning](https://www.tractica.com/automation-robotics/fpgas-challenge-gpus-as-a-platform-for-deep-learning/)
4.  2016/03 [FPGA with OpenCL Solution Released to Deep Learning](http://www.hpcwire.com/2016/03/17/fpga-opencl-solution-released-deep-learning/)
5.  2016/04 [Boosting Deep Learning with the Intel Scalable System Framework](http://www.nextplatform.com/2016/04/14/boosting-deep-learning-intel-scalable-system-framework/)
6.  2016/04 [Movidius puts deep learning chip in a USB drive](http://www.theverge.com/2016/4/28/11510430/movidius-fathom-neural-compute-stick-myriad-2-chip)
7.  2016/05 [The PCM-Neuron and Neural Computing](http://www.eetimes.com/author.asp?section_id=36&doc_id=1329754&)
8.  2016/05 [FPGA-accelerated deep convolutional neural networks for high throughput and energy efficiency](http://onlinelibrary.wiley.com/doi/10.1002/cpe.3850/full)

#### Hardware platforms & accelerators

1.  [Nvidia Devbox](https://developer.nvidia.com/devbox)
2.  [Google Tensor Processing Unit](http://www.anandtech.com/show/10340/googles-tensor-processing-unit-what-we-know)
3.  [Facebook Open Rack V2 compatible 8-GPU server](https://code.facebook.com/posts/1687861518126048/facebook-to-open-source-ai-hardware-design/)
4.  [CEVA DNN Digital Signal Processor](http://www.ceva-dsp.com/CDNN)
5.  [Movidius Fathom USB Stick](http://uploads.movidius.com/1463004959-Fathom-Combined-2-pager-web.pdf)
6.  [IBM TrueNorth](http://www.research.ibm.com/articles/brain-chip.shtml)
7.  [AMAX SenseBox](https://www.amax.com/about/pressdetail.asp?news_id=2016011204)

#### Licenses
License

[![CC0](http://i.creativecommons.org/p/zero/1.0/88x31.png)](http://creativecommons.org/publicdomain/zero/1.0/)

